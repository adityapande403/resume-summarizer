{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Representation \n",
    "\n",
    "The classifiers and learning algorithms can not directly process the text documents in their original form,as most of them expect numerical feature vectors with a fixed size rather than raw text docs with variable length. Therefore , during the preprocessing step, the texts are converted to a more manageable representation.\n",
    "\n",
    "One common approach for extracting features from text is to use the bag of words model: a model where for each document, a resume in our case, the presence (and often the frequency) of words is taken into consideration, but the order in which they occur is ignored. \n",
    "\n",
    "TermFrequency and InverseDocumentFrequency is used for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Resume</th>\n",
       "      <th>Advice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Science</td>\n",
       "      <td>Skills * Programming Languages: Python (pandas...</td>\n",
       "      <td>Use action verbs to start your resume bullet p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Science</td>\n",
       "      <td>Education Details \\r\\nMay 2013 to May 2017 B.E...</td>\n",
       "      <td>Customize your resume for each job application...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Science</td>\n",
       "      <td>Areas of Interest Deep Learning, Control Syste...</td>\n",
       "      <td>Highlight your achievements and quantify them ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Science</td>\n",
       "      <td>Skills Ã¢ÂÂ¢ R Ã¢ÂÂ¢ Python Ã¢ÂÂ¢ SAP HANA ...</td>\n",
       "      <td>Proofread your resume to eliminate spelling an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Science</td>\n",
       "      <td>Education Details \\r\\n MCA   YMCAUST,  Faridab...</td>\n",
       "      <td>Keep your resume concise; one to two pages is ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Category                                             Resume  \\\n",
       "0  Data Science  Skills * Programming Languages: Python (pandas...   \n",
       "1  Data Science  Education Details \\r\\nMay 2013 to May 2017 B.E...   \n",
       "2  Data Science  Areas of Interest Deep Learning, Control Syste...   \n",
       "3  Data Science  Skills Ã¢ÂÂ¢ R Ã¢ÂÂ¢ Python Ã¢ÂÂ¢ SAP HANA ...   \n",
       "4  Data Science  Education Details \\r\\n MCA   YMCAUST,  Faridab...   \n",
       "\n",
       "                                              Advice  \n",
       "0  Use action verbs to start your resume bullet p...  \n",
       "1  Customize your resume for each job application...  \n",
       "2  Highlight your achievements and quantify them ...  \n",
       "3  Proofread your resume to eliminate spelling an...  \n",
       "4  Keep your resume concise; one to two pages is ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "df = pd.read_csv('/Users/User/Downloads/UpdatedResumeDataSet.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# Handle and clean the data if necessary\n",
    "# For example, remove non-decodable characters or replace them with a placeholder\n",
    "df['Resume'] = df['Resume'].apply(lambda x: x.encode('ISO-8859-1', 'ignore').decode('ISO-8859-1'))\n",
    "resume_punc = df[\"Resume\"].copy(deep  = True)\n",
    "df.head()\n",
    "#resume_punc\n",
    "resume_punc = df[\"Resume\"].copy(deep  = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def rem_punc(s):\n",
    "    punc = string.punctuation\n",
    "    return [i for i in s if i not in punc]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "# Function to remove punctuation from a string\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "# Tokenize and remove punctuation for each row in the DataFrame\n",
    "for ind, row in df.iterrows():\n",
    "    text = row[\"Resume\"]\n",
    "    tokens = text.split()  # Split text into words\n",
    "    tokens_without_punctuation = [remove_punctuation(word) for word in tokens]\n",
    "    cleaned_text = \" \".join(tokens_without_punctuation)\n",
    "    df.at[ind, \"Resume\"] = cleaned_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from wordcloud import STOPWORDS\n",
    "def rem_punc(s):\n",
    "    punc = string.punctuation\n",
    "    return [i for i in s if i not in punc]\n",
    "\n",
    "def rem_sw(s):\n",
    "    sw = set(STOPWORDS)\n",
    "    return [i for i in s if i not in sw]\n",
    "\n",
    "def preprocess(eval_res):\n",
    "    try:\n",
    "        eval_res = eval(eval_res).decode()\n",
    "    except:\n",
    "        pass\n",
    "    eval_res = eval_res.encode(\"ASCII\",\"ignore\").decode()\n",
    "    length = len(eval_res)\n",
    "    eval_res = \" \".join(eval_res.split(\"\\n\"))\n",
    "    token = rem_sw(nltk.word_tokenize(eval_res)) #Removing punctaution later since we need punctaution for sentence tokenization\n",
    "    eval_res = \" \".join(token).lower()\n",
    "    return eval_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning data and adding in ID for category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Resume</th>\n",
       "      <th>category_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Science</td>\n",
       "      <td>Skills  Programming Languages Python pandas nu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Science</td>\n",
       "      <td>Education Details May 2013 to May 2017 BE UITR...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Science</td>\n",
       "      <td>Areas of Interest Deep Learning Control System...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Science</td>\n",
       "      <td>Skills Ã¢ÂÂ¢ R Ã¢ÂÂ¢ Python Ã¢ÂÂ¢ SAP HANA ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Science</td>\n",
       "      <td>Education Details MCA YMCAUST Faridabad Haryan...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Category                                             Resume  \\\n",
       "0  Data Science  Skills  Programming Languages Python pandas nu...   \n",
       "1  Data Science  Education Details May 2013 to May 2017 BE UITR...   \n",
       "2  Data Science  Areas of Interest Deep Learning Control System...   \n",
       "3  Data Science  Skills Ã¢ÂÂ¢ R Ã¢ÂÂ¢ Python Ã¢ÂÂ¢ SAP HANA ...   \n",
       "4  Data Science  Education Details MCA YMCAUST Faridabad Haryan...   \n",
       "\n",
       "   category_id  \n",
       "0            0  \n",
       "1            0  \n",
       "2            0  \n",
       "3            0  \n",
       "4            0  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from io import StringIO\n",
    "col = ['Category', 'Resume']\n",
    "df = df[col]\n",
    "df = df[pd.notnull(df['Resume'])]\n",
    "df.columns = ['Category', 'Resume']\n",
    "df['category_id'] = df['Category'].factorize()[0]\n",
    "category_id_df = df[['Category', 'category_id']].drop_duplicates().sort_values('category_id')\n",
    "category_to_id = dict(category_id_df.values)\n",
    "id_to_category = dict(category_id_df[['category_id', 'Category']].values)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(962, 27702)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1,2), stop_words='english')\n",
    "features = tfidf.fit_transform(df.Resume).toarray()\n",
    "labels = df.category_id\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using chi2 to see correlated items:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 'Advocate':\n",
      "  . Most correlated unigrams:\n",
      "\t. law\n",
      "\t. advocate\n",
      "  . Most correlated bigrams:\n",
      "\t. university advocate\n",
      "\t. advocate skill\n",
      "\n",
      "\n",
      "\n",
      "# 'Arts':\n",
      "  . Most correlated unigrams:\n",
      "\t. karate\n",
      "\t. arts\n",
      "  . Most correlated bigrams:\n",
      "\t. drawing arts\n",
      "\t. craft teacher\n",
      "\n",
      "\n",
      "\n",
      "# 'Automation Testing':\n",
      "  . Most correlated unigrams:\n",
      "\t. selenium\n",
      "\t. box\n",
      "  . Most correlated bigrams:\n",
      "\t. automation testing\n",
      "\t. manual automation\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 'Blockchain':\n",
      "  . Most correlated unigrams:\n",
      "\t. ethereum\n",
      "\t. blockchain\n",
      "  . Most correlated bigrams:\n",
      "\t. smart contracts\n",
      "\t. blockchain developer\n",
      "\n",
      "\n",
      "\n",
      "# 'Business Analyst':\n",
      "  . Most correlated unigrams:\n",
      "\t. mms\n",
      "\t. analyst\n",
      "  . Most correlated bigrams:\n",
      "\t. analyst business\n",
      "\t. business analyst\n",
      "\n",
      "\n",
      "\n",
      "# 'Civil Engineer':\n",
      "  . Most correlated unigrams:\n",
      "\t. construction\n",
      "\t. civil\n",
      "  . Most correlated bigrams:\n",
      "\t. site engineer\n",
      "\t. civil engineer\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 'Data Science':\n",
      "  . Most correlated unigrams:\n",
      "\t. deep\n",
      "\t. learning\n",
      "  . Most correlated bigrams:\n",
      "\t. machine learning\n",
      "\t. data science\n",
      "\n",
      "\n",
      "\n",
      "# 'Database':\n",
      "  . Most correlated unigrams:\n",
      "\t. dba\n",
      "\t. rman\n",
      "  . Most correlated bigrams:\n",
      "\t. database administration\n",
      "\t. database administrator\n",
      "\n",
      "\n",
      "\n",
      "# 'DevOps Engineer':\n",
      "  . Most correlated unigrams:\n",
      "\t. aes\n",
      "\t. devops\n",
      "  . Most correlated bigrams:\n",
      "\t. size role\n",
      "\t. devops engineer\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 'DotNet Developer':\n",
      "  . Most correlated unigrams:\n",
      "\t. net\n",
      "\t. dot\n",
      "  . Most correlated bigrams:\n",
      "\t. net developer\n",
      "\t. dot net\n",
      "\n",
      "\n",
      "\n",
      "# 'ETL Developer':\n",
      "  . Most correlated unigrams:\n",
      "\t. informatica\n",
      "\t. etl\n",
      "  . Most correlated bigrams:\n",
      "\t. power center\n",
      "\t. etl developer\n",
      "\n",
      "\n",
      "\n",
      "# 'Electrical Engineering':\n",
      "  . Most correlated unigrams:\n",
      "\t. fighting\n",
      "\t. electrical\n",
      "  . Most correlated bigrams:\n",
      "\t. electrical engineering\n",
      "\t. power plant\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 'HR':\n",
      "  . Most correlated unigrams:\n",
      "\t. ba\n",
      "\t. hr\n",
      "  . Most correlated bigrams:\n",
      "\t. university hr\n",
      "\t. hr skill\n",
      "\n",
      "\n",
      "\n",
      "# 'Hadoop':\n",
      "  . Most correlated unigrams:\n",
      "\t. hive\n",
      "\t. hadoop\n",
      "  . Most correlated bigrams:\n",
      "\t. map reduce\n",
      "\t. hadoop developer\n",
      "\n",
      "\n",
      "\n",
      "# 'Health and fitness':\n",
      "  . Most correlated unigrams:\n",
      "\t. nutrition\n",
      "\t. fitness\n",
      "  . Most correlated bigrams:\n",
      "\t. fitness trainer\n",
      "\t. health fitness\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 'Java Developer':\n",
      "  . Most correlated unigrams:\n",
      "\t. j2ee\n",
      "\t. spring\n",
      "  . Most correlated bigrams:\n",
      "\t. developer java\n",
      "\t. java developer\n",
      "\n",
      "\n",
      "\n",
      "# 'Mechanical Engineer':\n",
      "  . Most correlated unigrams:\n",
      "\t. mesa\n",
      "\t. mechanical\n",
      "  . Most correlated bigrams:\n",
      "\t. design engineer\n",
      "\t. mechanical engineering\n",
      "\n",
      "\n",
      "\n",
      "# 'Network Security Engineer':\n",
      "  . Most correlated unigrams:\n",
      "\t. firewall\n",
      "\t. cisco\n",
      "  . Most correlated bigrams:\n",
      "\t. security exprience\n",
      "\t. network security\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 'Operations Manager':\n",
      "  . Most correlated unigrams:\n",
      "\t. contributions\n",
      "\t. prepaid\n",
      "  . Most correlated bigrams:\n",
      "\t. manager operations\n",
      "\t. operations manager\n",
      "\n",
      "\n",
      "\n",
      "# 'PMO':\n",
      "  . Most correlated unigrams:\n",
      "\t. transition\n",
      "\t. pmo\n",
      "  . Most correlated bigrams:\n",
      "\t. school delhi\n",
      "\t. business administration\n",
      "\n",
      "\n",
      "\n",
      "# 'Python Developer':\n",
      "  . Most correlated unigrams:\n",
      "\t. django\n",
      "\t. python\n",
      "  . Most correlated bigrams:\n",
      "\t. developer python\n",
      "\t. python developer\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 'SAP Developer':\n",
      "  . Most correlated unigrams:\n",
      "\t. abap\n",
      "\t. sap\n",
      "  . Most correlated bigrams:\n",
      "\t. sap bo\n",
      "\t. months sap\n",
      "\n",
      "\n",
      "\n",
      "# 'Sales':\n",
      "  . Most correlated unigrams:\n",
      "\t. honda\n",
      "\t. calling\n",
      "  . Most correlated bigrams:\n",
      "\t. sales exprience\n",
      "\t. sales manager\n",
      "\n",
      "\n",
      "\n",
      "# 'Testing':\n",
      "  . Most correlated unigrams:\n",
      "\t. transformer\n",
      "\t. pcb\n",
      "  . Most correlated bigrams:\n",
      "\t. electronics pvt\n",
      "\t. testing engineer\n",
      "\n",
      "\n",
      "\n",
      "# 'Web Designing':\n",
      "  . Most correlated unigrams:\n",
      "\t. graphics\n",
      "\t. allahabad\n",
      "  . Most correlated bigrams:\n",
      "\t. roles responsibility\n",
      "\t. web designer\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "import numpy as np\n",
    "N = 2\n",
    "for Category, category_id in sorted(category_to_id.items()):\n",
    "    features_chi2 = chi2(features, labels == category_id)\n",
    "    indices = np.argsort(features_chi2[0])\n",
    "    feature_names = np.array(tfidf.get_feature_names())[indices]\n",
    "    #print(feature_names)\n",
    "    unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
    "    bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
    "    #trigrams = [v for v in feature_names if len(v.split(' ')) == 3] \n",
    "    print(\"# '{}':\".format(Category))\n",
    "    print(\"  . Most correlated unigrams:\\n\\t. {}\".format('\\n\\t. '.join(unigrams[-N:])))\n",
    "    print(\"  . Most correlated bigrams:\\n\\t. {}\".format('\\n\\t. '.join(bigrams[-N:])))\n",
    "    print(\"\\n\\n\")\n",
    "    #print(\"  . Most correlated trigrams:\\n. {}\".format('\\n. '.join(trigrams[-N:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding Why :** \n",
    "\n",
    "Suppose there are N instances, and two classes(say A and B).Given a feature X, we can use Chi Square Test to evaluate its importance to distinguish between the classes. \n",
    "By calculating the Chi square scores for all the features, we can rank the features by the chi square scores, then choose the top ranked features for model training. \n",
    "**Chi Square Test is used in statistics to test the independence of two events.\n",
    "In feature selection part of this project , the two events are :** \n",
    "\n",
    "**1.Occurence of a feature**\n",
    "\n",
    "**2.Occurence of a Class/Doc category** \n",
    "\n",
    "**Note:** \n",
    "the higher value of the chi^2 score, the more likelihood the feature is correlated with the class, thus it should be selected for model training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Class Classifier: Features and Design\n",
    "\n",
    "To train supervised classifiers, we first transformed the “Resumes” into a vector of numbers. We explored vector representations such as TF-IDF weighted vectors and also made sure there is some kind of correlation using the Chi^2 test to confirm that predictions are possible with these features that can be extracted from the documents. \n",
    "\n",
    "After having this vector representations of the text we can train supervised classifiers to train unseen “Resumes” and predict the “Job Category” on which they fall. After all the above data transformation, now that we have all the features and labels, it is time to train the classifiers. There are a number of algorithms we can use for this type of problem. \n",
    "\n",
    "\n",
    "Naive Bayes Classifier: the one most suitable for word counts is the multinomial variant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df['Resume'], df['Category'], random_state = 0)\n",
    "\n",
    "#print(x_train)\n",
    "\n",
    "count_vect = CountVectorizer() # bag-of-ngrams model , based on frequency count\n",
    "x_train_counts = count_vect.fit_transform(x_train)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer() #passing the word:word count\n",
    "x_train_tfidf = tfidf_transformer.fit_transform(x_train_counts)\n",
    "\n",
    "classifier = MultinomialNB().fit(x_train_tfidf, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarization Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "from string import punctuation\n",
    "import math\n",
    "def summarize(doc,words):\n",
    "    score={}\n",
    "    fd = FreqDist(words)\n",
    "    for i,t in enumerate(doc):\n",
    "        score[i] = 0\n",
    "        for j in nltk.word_tokenize(t):\n",
    "            if j in fd:\n",
    "                score[i]+=fd[j]\n",
    "    \n",
    "    r = sorted(list(score.items()),key=lambda x:x[1],reverse=True)[:math.floor(0.60*len(doc))]\n",
    "    r.sort(key=lambda x:x[0])\n",
    "    l = [doc[i[0]] for i in r]\n",
    "    return \"\\n\\n\".join(l)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advice Generation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Advice'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/indexes/base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3620\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/_libs/index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/_libs/index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Advice'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [94]\u001b[0m, in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m     22\u001b[0m     resume_tokens \u001b[38;5;241m=\u001b[39m tokenize_text(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mResume\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 23\u001b[0m     advice_tokens \u001b[38;5;241m=\u001b[39m tokenize_text(\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAdvice\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m resume_tokens:\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m resume_vocab:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/series.py:958\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    955\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m--> 958\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_hashable(key):\n\u001b[1;32m    961\u001b[0m     \u001b[38;5;66;03m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[1;32m    962\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    963\u001b[0m         \u001b[38;5;66;03m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/series.py:1069\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1066\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1068\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1069\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39m_get_values_for_loc(\u001b[38;5;28mself\u001b[39m, loc, label)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/indexes/base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3623\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3624\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3625\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3626\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3627\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3628\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Advice'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import spacy\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Load English tokenizer, tagger, parser, NER, and word vectors\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Define a function to preprocess text and tokenize it using spaCy\n",
    "def tokenize_text(text):\n",
    "    return [tok.text.lower() for tok in nlp(text)]\n",
    "\n",
    "\n",
    "\n",
    "# Tokenize and build the vocabulary\n",
    "resume_vocab = {\"<sos>\": 0, \"<eos>\": 1}  \n",
    "advice_vocab = {\"<sos>\": 0, \"<eos>\": 1}\n",
    "resume_vocab_idx = 2\n",
    "advice_vocab_idx = 2\n",
    "for index, row in df.iterrows():\n",
    "    resume_tokens = tokenize_text(row['Resume'])\n",
    "    advice_tokens = tokenize_text(row['Advice'])\n",
    "\n",
    "    for token in resume_tokens:\n",
    "        if token not in resume_vocab:\n",
    "            resume_vocab[token] = resume_vocab_idx\n",
    "            resume_vocab_idx += 1\n",
    "    for token in advice_tokens:\n",
    "        if token not in advice_vocab:\n",
    "            advice_vocab[token] = advice_vocab_idx\n",
    "            advice_vocab_idx += 1\n",
    "            \n",
    "# Encoder class\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, hid_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(hid_dim, hid_dim, n_layers, dropout=dropout) \n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        \n",
    "        embedded = self.embedding(src)\n",
    "        \n",
    "        #embedded = [src len, batch size, hid dim]\n",
    "        \n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        \n",
    "        #outputs = [src len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #outputs are always from the top hidden layer\n",
    "        \n",
    "        return hidden, cell\n",
    "        \n",
    "# Decoder class\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, hid_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(hid_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        \n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        \n",
    "    def forward(self, input, hidden, cell):\n",
    "             \n",
    "        #input = [batch size]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #n directions in the decoder will both always be 1\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        #input = [1, batch size]\n",
    "        \n",
    "        embedded = self.embedding(input)\n",
    "        \n",
    "        #embedded = [1, batch size, hid dim]\n",
    "        \n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        \n",
    "        #output = [seq len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #seq len and n directions will always be 1 in the decoder\n",
    "        #therefore:\n",
    "        #output = [1, batch size, hid dim]\n",
    "        \n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        \n",
    "        #prediction = [batch size, output dim]\n",
    "        \n",
    "        return prediction, hidden, cell\n",
    "\n",
    "# Define the sequence-to-sequence model\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, enc_hidden_dim, dec_hidden_dim, enc_num_layers, dec_num_layers, enc_dropout, dec_dropout):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = Encoder(input_dim, enc_hidden_dim, enc_num_layers, enc_dropout)\n",
    "        self.decoder = Decoder(output_dim, dec_hidden_dim, dec_num_layers, dec_dropout)\n",
    "        self.device = device\n",
    "    \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "\n",
    "          # src: input sequence (resume)\n",
    "        # trg: target sequence (advice)\n",
    "        # teacher_forcing_ratio: probability of using teacher forcing\n",
    "        \n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.embedding.num_embeddings\n",
    "\n",
    "        # Initialize a tensor to store the outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "\n",
    "        # Pass the source sequence through the encoder\n",
    "        enc_hidden, enc_cell = self.encoder(src)\n",
    "\n",
    "        # Initial input to the decoder is the <sos> token\n",
    "        input = trg[0, :]\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            output, enc_hidden, enc_cell = self.decoder(input, enc_hidden, enc_cell)\n",
    "\n",
    "            # Store the output\n",
    "            outputs[t] = output\n",
    "\n",
    "            # Decide whether to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "\n",
    "            # Get the highest predicted token from the output\n",
    "            top1 = output.argmax(1)\n",
    "\n",
    "            # If teacher forcing, use the actual next token as the next input\n",
    "            # If not, use the predicted token\n",
    "            input = trg[t] if teacher_force else top1\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "# Create the encoder and decoder \n",
    "enc = Encoder(input_dim, enc_hidden_dim, enc_num_layers, enc_dropout)\n",
    "dec = Decoder(output_dim, dec_hidden_dim, dec_num_layers, dec_dropout)\n",
    "input_dim = len(resume_vocab)\n",
    "output_dim = len(advice_vocab)\n",
    "enc_hidden_dim = 256\n",
    "dec_hidden_dim = 256  \n",
    "enc_num_layers = 2\n",
    "dec_num_layers = 2\n",
    "enc_dropout = 0.5\n",
    "dec_dropout = 0.5\n",
    "\n",
    "# Create the Seq2Seq model\n",
    "model = Seq2Seq(input_dim, output_dim, enc_hidden_dim, dec_hidden_dim, enc_num_layers, dec_num_layers, enc_dropout, dec_dropout)\n",
    "\n",
    "# Define the training loop and loss function\n",
    "# ...\n",
    "\n",
    "# Train the model on the training df \n",
    "# ...\n",
    "\n",
    "# Define a function to generate advice from a resume\n",
    "def generate_advice_from_resume(model, resume, max_len=50):\n",
    "\n",
    "    model.eval()  \n",
    "    tokenized_resume = tokenize_text(resume)\n",
    "    tokenized_resume = [\"<sos>\"] + tokenized_resume + [\"<eos>\"]\n",
    "    numericalized_resume = [resume_vocab.get(token, resume_vocab[\"<sos>\"]) for token in tokenized_resume] \n",
    "    sentence_length = [len(numericalized_resume)]\n",
    "    input_tensor = torch.LongTensor(numericalized_resume).unsqueeze(1).to(device)\n",
    "    \n",
    "    outputs = [advice_vocab[\"<sos>\"]]\n",
    "    \n",
    "    for i in range(max_len):\n",
    "\n",
    "        trg_tensor = torch.LongTensor(outputs).unsqueeze(1).to(device)\n",
    "        output = model(input_tensor, trg_tensor, 0) # Turn off teacher forcing\n",
    "        top_token = output.argmax(2)[-1]\n",
    "        if top_token.item() == advice_vocab[\"<eos>\"]:\n",
    "            break\n",
    "        outputs.append(top_token.item())\n",
    "\n",
    "    generated_advice = [token for token in advice_vocab if advice_vocab[token] in outputs]\n",
    "    generated_advice = generated_advice[1:] # Remove the starting token\n",
    "\n",
    "    return \" \".join(generated_advice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing it on an unseen pdf resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Haryana, India \n",
      "\n",
      "       laulrushil@gmail.com \n",
      "\n",
      "       +91   9871934103   \n",
      "\n",
      "RUSHIL LAUL \n",
      "\n",
      "Date of birth \n",
      "Place of birth \n",
      "E - Mail   \n",
      "\n",
      "21.08.1999   \n",
      "Delhi,   India   \n",
      "laulrushil@gmail.com \n",
      "\n",
      "OBJECTIVE:  \n",
      "\n",
      " EDUCATION  \n",
      "\n",
      "•  Seeking a Software  Engineer  position to apply my analytical skills and the  knowledge I have \n",
      "gained  through  my  degree  in  Software  Engineering.  Eager  to  join  a  dynamic  and  forward-\n",
      "thinking organization that maximizes the utilization of my skills. \n",
      "\n",
      "2021-2023 \n",
      "\n",
      "M.E (Master of engineering) – Software engineering  \n",
      "\n",
      "University: Thapar University, Patiala \n",
      "\n",
      "      2017-2021                B.Tech Computer Science  \n",
      "\n",
      "University: Manav Rachna International Institute of Research and Studies \n",
      "\n",
      " WORK EXPOSURE  \n",
      "\n",
      "07.2022 – 06.2023  \n",
      "\n",
      "SIEMENS, INDIA  \n",
      "\n",
      "•  Extensive customization to customers with installation support.  \n",
      "\n",
      "•  Oracle database management and customization. \n",
      "\n",
      "•  Creating and testing environments on Linux and windows operating systems. \n",
      "\n",
      "•  Preparing start to end documentation for whole team as well as customers. \n",
      "\n",
      "•  Configuration of micro services for AWC with Docker and Docker registry on Linux. \n",
      "\n",
      "• \n",
      "\n",
      "JBoss, WebLogic configuration and for Teamcenter \n",
      "\n",
      " Technical Trainings \n",
      "\n",
      "•  Python full stack Training From DUCAT, Noida \n",
      "•  Full stack Data science course from TechnoGeeks Pune \n",
      "•  Entrepreneurship workshops \n",
      "•  Attended 5 days’ workshop‘ Open-Source Platforms for Internet Of Things \n",
      "\n",
      "and Application’ (30th July, 2018- 3rd August, 2018)’organized by \n",
      "MRIIRS. \n",
      "\n",
      " Projects \n",
      "\n",
      "•  Student Management System: Python Based prototype which manages \n",
      "\n",
      "students in a university or a school \n",
      "\n",
      "•  Sign Language Recognition System: Based on MACHINE LEARNING. It \n",
      "\n",
      "uses signs and find perfect word. \n",
      "\n",
      "•  Shop Management:  Java based prototype which manages all the items \n",
      "\n",
      "in the shop. \n",
      "\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      " \n",
      "  \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      "  \n",
      "  \n",
      "  \n",
      " \n",
      "\f",
      "COMPETENCE AND SKILLS  \n",
      "\n",
      "Python , Linux ,C, MySQL , SQLite, C++, Java, html, Numpy, Pandas, Software diagnosis, Decision making, \n",
      "Tkinter, Artificial Intelligence, Machine Learning, Data-Analytics, Technical Analysis, Multi-tasking, Flask , MS \n",
      "Outlook, SQL, Excel \n",
      "\n",
      " LANGUAGE SKILLS  \n",
      "English (business proficiency), Hindi, Punjabi  \n",
      "\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "\n",
    "def convertPDFtoText(path):\n",
    "    output_string = io.StringIO()\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, output_string, laparams=laparams)\n",
    "    \n",
    "    with open(path, 'rb') as fp:\n",
    "        interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "        for page in PDFPage.get_pages(fp):\n",
    "            interpreter.process_page(page)\n",
    "    \n",
    "    text = output_string.getvalue()\n",
    "    \n",
    "    device.close()\n",
    "    output_string.close()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Example usage:\n",
    "test_resume = convertPDFtoText(\"/Users/User/Downloads/CV_Rushil_.pdf\")\n",
    "print(test_resume)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We pass the resume extracted from the pdf using OCR through preprocess function to bring it down to the same state as the trained data, and use this for classification and summarization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Test Resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Haryana, India \n",
      "\n",
      "       laulrushil@gmail.com \n",
      "\n",
      "       +91   9871934103   \n",
      "\n",
      "RUSHIL LAUL \n",
      "\n",
      "Date of birth \n",
      "Place of birth \n",
      "E - Mail   \n",
      "\n",
      "21.08.1999   \n",
      "Delhi,   India   \n",
      "laulrushil@gmail.com \n",
      "\n",
      "OBJECTIVE:  \n",
      "\n",
      " EDUCATION  \n",
      "\n",
      "•  Seeking a Software  Engineer  position to apply my analytical skills and the  knowledge I have \n",
      "gained  through  my  degree  in  Software  Engineering.\n",
      "\n",
      "Eager  to  join  a  dynamic  and  forward-\n",
      "thinking organization that maximizes the utilization of my skills.\n",
      "\n",
      "2021-2023 \n",
      "\n",
      "M.E (Master of engineering) – Software engineering  \n",
      "\n",
      "University: Thapar University, Patiala \n",
      "\n",
      "      2017-2021                B.Tech Computer Science  \n",
      "\n",
      "University: Manav Rachna International Institute of Research and Studies \n",
      "\n",
      " WORK EXPOSURE  \n",
      "\n",
      "07.2022 – 06.2023  \n",
      "\n",
      "SIEMENS, INDIA  \n",
      "\n",
      "•  Extensive customization to customers with installation support.\n",
      "\n",
      "• \n",
      "\n",
      "JBoss, WebLogic configuration and for Teamcenter \n",
      "\n",
      " Technical Trainings \n",
      "\n",
      "•  Python full stack Training From DUCAT, Noida \n",
      "•  Full stack Data science course from TechnoGeeks Pune \n",
      "•  Entrepreneurship workshops \n",
      "•  Attended 5 days’ workshop‘ Open-Source Platforms for Internet Of Things \n",
      "\n",
      "and Application’ (30th July, 2018- 3rd August, 2018)’organized by \n",
      "MRIIRS.\n",
      "\n",
      "Projects \n",
      "\n",
      "•  Student Management System: Python Based prototype which manages \n",
      "\n",
      "students in a university or a school \n",
      "\n",
      "•  Sign Language Recognition System: Based on MACHINE LEARNING.\n",
      "\n",
      "•  Shop Management:  Java based prototype which manages all the items \n",
      "\n",
      "in the shop.\n",
      "\n",
      "COMPETENCE AND SKILLS  \n",
      "\n",
      "Python , Linux ,C, MySQL , SQLite, C++, Java, html, Numpy, Pandas, Software diagnosis, Decision making, \n",
      "Tkinter, Artificial Intelligence, Machine Learning, Data-Analytics, Technical Analysis, Multi-tasking, Flask , MS \n",
      "Outlook, SQL, Excel \n",
      "\n",
      " LANGUAGE SKILLS  \n",
      "English (business proficiency), Hindi, Punjabi\n"
     ]
    }
   ],
   "source": [
    "resume = preprocess(test_resume)#remove stop words etc\n",
    "sent = nltk.sent_tokenize(test_resume)\n",
    "\n",
    "word_token = nltk.word_tokenize(test_resume)#tokenize preprocessed text for scoring\n",
    "\n",
    "print(summarize(sent,test_resume))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicted Label for Test Resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Java Developer']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classifier.predict(count_vect.transform([test_resume])))\n",
    "generated_advice = generate_advice_from_resume(model, test_resume)\n",
    "print(generated_advice)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparisons between models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "models = [\n",
    "    RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0),\n",
    "    LinearSVC(),\n",
    "    MultinomialNB(),\n",
    "    LogisticRegression(random_state=0),\n",
    "]\n",
    "\n",
    "\n",
    "CV = 5\n",
    "cv_df = pd.DataFrame(index=range(CV * len(models)))\n",
    "entries = []\n",
    "for model in models:\n",
    "    model_name = model.__class__.__name__\n",
    "    accuracies = cross_val_score(model, features, labels, scoring='accuracy', cv=CV)\n",
    "    for fold_idx, accuracy in enumerate(accuracies):\n",
    "        entries.append((model_name, fold_idx, accuracy))\n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.boxplot(x='model_name', y='accuracy', data=cv_df)\n",
    "sns.stripplot(x='model_name', y='accuracy', data=cv_df, \n",
    "              size=8, jitter=True, edgecolor=\"gray\", linewidth=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_df.groupby('model_name').accuracy.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe an SVM and the Logistic regression models seem to be doing better with accuracy of around 60-70%.\n",
    "\n",
    "\n",
    "\n",
    "### Linear SVC:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearSVC()\n",
    "X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(features, labels, df.index, test_size=0.30, random_state=0)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d',\n",
    "            xticklabels=category_id_df.Category.values, yticklabels=category_id_df.Category.values)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As we can clearly see from the visualization that a vast majority of the predicted values lie on the diagonal representing True Positive values**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
